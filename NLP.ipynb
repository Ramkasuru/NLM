{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1HGYyAidMdQkndcx9CQkTCBAzYwmQ6fOb",
      "authorship_tag": "ABX9TyN4NWS+FnzstZWfwAkP1f/w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramkasuru/NLM/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Y1q9ujIuB6xe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JM9_6ul2jrS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import Word\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from collections import Counter\n"
      ],
      "metadata": {
        "id": "TU0rwKBa-u7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G3qljfuk3jF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEaxNKTI3x22",
        "outputId": "9fba5a99-7486-4ece-dcb5-084e17ea14ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/NLP dataset/train_add.csv')\n"
      ],
      "metadata": {
        "id": "nT1Yj7iZ4mEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# replace 'filename.csv' with the name of your CSV file\n",
        "# Compute word frequency in filtered dataset\n",
        "word_frequency = {}\n",
        "for sentence in filtered_dataset:\n",
        "    tokens = sentence.split()\n",
        "    for token in tokens:\n",
        "        if token not in word_frequency:\n",
        "            word_frequency[token] = 1\n",
        "        else:\n",
        "            word_frequency[token] += 1\n",
        "\n",
        "print(word_frequency)\n",
        "\n",
        "# print the contents of the CSV file\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFrFhU0l7oQu",
        "outputId": "7791432f-e22c-42a9-ff16-2fdf619784ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           id  keyword  location  \\\n",
            "0  2237307600      NaN       NaN   \n",
            "1  2001169708      NaN       NaN   \n",
            "2  1932411345      NaN       NaN   \n",
            "3  2050120420      NaN       NaN   \n",
            "4  1957522146      NaN       NaN   \n",
            "\n",
            "                                                text  target  \n",
            "0                               it is raining again        0  \n",
            "1  We had baseball game on Sunday we lose 2-0 :/ ...       0  \n",
            "2  All my prayers goes out to Rodney Rodgers and ...       1  \n",
            "3  good news!! i got a bike!! bad news... my car ...       0  \n",
            "4   Sakasama no Chou - Upside down butterfly.....?!        0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['id', 'keyword', 'location'], axis=1)\n"
      ],
      "metadata": {
        "id": "SB8iYB-L8YpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYiHuhaF87sm",
        "outputId": "b847c2ec-45c8-4255-fc53-e5fbe68e9cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  target\n",
            "0                               it is raining again        0\n",
            "1  We had baseball game on Sunday we lose 2-0 :/ ...       0\n",
            "2  All my prayers goes out to Rodney Rodgers and ...       1\n",
            "3  good news!! i got a bike!! bad news... my car ...       0\n",
            "4   Sakasama no Chou - Upside down butterfly.....?!        0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['target'], axis=1)"
      ],
      "metadata": {
        "id": "Fli9IsNM9F9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3p-1smwE9MbV",
        "outputId": "b2fb0255-340e-4a9e-f2a9-0441309fc7d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text\n",
            "0                               it is raining again \n",
            "1  We had baseball game on Sunday we lose 2-0 :/ ...\n",
            "2  All my prayers goes out to Rodney Rodgers and ...\n",
            "3  good news!! i got a bike!! bad news... my car ...\n",
            "4   Sakasama no Chou - Upside down butterfly.....?! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
        "df['text'] = df['text'].apply(lambda x: re.sub(r'@\\S+', '', x))\n",
        "df['text'] = df['text'].apply(lambda x: re.sub(r'#\\S+', '', x))\n"
      ],
      "metadata": {
        "id": "GzqxTq6y-5UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization\n"
      ],
      "metadata": {
        "id": "PSRVVB4M_2lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KkFKuR2F_5oL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['text'].apply(lambda x: x.lower())\n"
      ],
      "metadata": {
        "id": "grsR2KYn-8Pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "lower() function\n"
      ],
      "metadata": {
        "id": "L78BybOR_9Fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['text_tokens'] = data['text'].apply(lambda x: re.findall(r'\\b\\w+\\b', x))\n"
      ],
      "metadata": {
        "id": "iSwd2hbI5Yw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Findall()"
      ],
      "metadata": {
        "id": "r8i91wj2BZHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncPPusf8BjL_",
        "outputId": "f32969dd-fa5b-4467-dc26-f4a573240026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text\n",
            "0                               it is raining again \n",
            "1  we had baseball game on sunday we lose 2-0 :/ ...\n",
            "2  all my prayers goes out to rodney rodgers and ...\n",
            "3  good news!! i got a bike!! bad news... my car ...\n",
            "4   sakasama no chou - upside down butterfly.....?! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "url = 'https://www.topendsports.com/world/lists/top-athletes/index.htm'\n",
        "res = requests.get(url)\n",
        "soup = BeautifulSoup(res.content, 'html.parser')\n",
        "\n",
        "athlete_data = []\n",
        "\n",
        "for row in soup.find_all('tr'):\n",
        "    athlete = row.find_all('td')\n",
        "    if len(athlete) == 3:\n",
        "        athlete_data.append({\n",
        "            'rank': athlete[0].text.strip(),\n",
        "            'name': athlete[1].text.strip(),\n",
        "            'sport': athlete[2].text.strip()\n",
        "        })\n",
        "\n",
        "athlete_df = pd.DataFrame(athlete_data)"
      ],
      "metadata": {
        "id": "AvW1XWPy5bjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = r'\\b\\w+\\b'\n",
        "\n",
        "# Apply the regular expression to the 'text' column of the DataFrame\n",
        "tokens = df['text'].apply(lambda x: re.findall(pattern, x))\n",
        "\n",
        "# Flatten the list of tokens\n",
        "tokens = [word for tweet in tokens for word in tweet]\n",
        "\n",
        "# Print the first 10 tokens\n",
        "print(tokens[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Oue_ToV-ADX",
        "outputId": "99d220b5-c59a-4f07-8e25-f0b8de127397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['it', 'is', 'raining', 'again', 'we', 'had', 'baseball', 'game', 'on', 'sunday']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deuRURE-6f8V",
        "outputId": "c3d489f1-7f5d-462f-816e-d67bc26c9f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "data['text_normalized'] = data['text'].apply(lambda x: ' '.join(word.lower() for word in x.split() if word.lower() not in stop_words))\n"
      ],
      "metadata": {
        "id": "hwCspuuT5kG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['text'].apply(lambda x: x.replace(\"u.s.\", \"united states\").replace(\"n't\", \" not\").replace(\"'m\", \" am\"))\n"
      ],
      "metadata": {
        "id": "ySIW9asH_KmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwnEOj_G8Svn",
        "outputId": "f57a5248-d589-4004-acbf-b61b2384a6e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text\n",
            "0                               it is raining again \n",
            "1  we had baseball game on sunday we lose 2-0 :/ ...\n",
            "2  all my prayers goes out to rodney rodgers and ...\n",
            "3  good news!! i got a bike!! bad news... my car ...\n",
            "4   sakasama no chou - upside down butterfly.....?! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['text'].apply(lambda x: ' '.join([Word(word).lemmatize() for word in x.split()]))\n"
      ],
      "metadata": {
        "id": "gh-bNhZvAH1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abbreviations = {\n",
        "    'u': 'you',\n",
        "    'r': 'are',\n",
        "    'c': 'see',\n",
        "    '2': 'to',\n",
        "    '4': 'for',\n",
        "    'b': 'be',\n",
        "    'w': 'with',\n",
        "    'y': 'why',\n",
        "    'thx': 'thanks',\n",
        "    'lol': 'laughing out loud',\n",
        "    'imo': 'in my opinion',\n",
        "    'imho': 'in my humble opinion'\n",
        "}\n",
        "\n",
        "data['text_standardized'] = data['text_normalized'].apply(lambda x: ' '.join(abbreviations.get(word, word) for word in x.split()))\n"
      ],
      "metadata": {
        "id": "47VRaIOr5qWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIWZLUHn64Zl",
        "outputId": "0a08e609-6e4a-4f88-dca5-c51af7da6a65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "data['text_lemmatized'] = data['text_standardized'].apply(lambda x: ' '.join(Word(word).lemmatize() for word in x.split()))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIeVgS-L5xrm",
        "outputId": "d70566ff-9d2b-4f2c-c7e0-8a34469108b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [word for tweet in df['text'] for word in tweet.split()]\n",
        "frequency = Counter(tokens)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lnd8J7ld6G7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-6FlPnBe_QZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJy2Xo1g_krW",
        "outputId": "6bf938e4-04b2-437b-b3a4-7356d73504a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text\n",
            "0                                it is raining again\n",
            "1  we had baseball game on sunday we lose 2-0 :/ ...\n",
            "2  all my prayer go out to rodney rodgers and his...\n",
            "3  good news!! i got a bike!! bad news... my car ...\n",
            "4    sakasama no chou - upside down butterfly.....?!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define regular expression to remove non-alphanumeric characters and convert to lowercase\n",
        "regex = re.compile('[^a-zA-Z0-9 ]')"
      ],
      "metadata": {
        "id": "-ykWB1KNCkW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute word frequency in filtered dataset\n",
        "word_frequency = {}\n",
        "for sentence in filtered_dataset:\n",
        "    tokens = sentence.split()\n",
        "    for token in tokens:\n",
        "        if token not in word_frequency:\n",
        "            word_frequency[token] = 1\n",
        "        else:\n",
        "            word_frequency[token] += 1\n",
        "\n",
        "print(word_frequency)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FE09gQlEPiu",
        "outputId": "158db873-28c7-43d9-e280-84b3a331554b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PF2QQd0-ERtG",
        "outputId": "9a5b08c7-3fa6-4c49-ddb4-e661b8caa017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text\n",
            "0                                it is raining again\n",
            "1  we had baseball game on sunday we lose 2-0 :/ ...\n",
            "2  all my prayer go out to rodney rodgers and his...\n",
            "3  good news!! i got a bike!! bad news... my car ...\n",
            "4    sakasama no chou - upside down butterfly.....?!\n"
          ]
        }
      ]
    }
  ]
}